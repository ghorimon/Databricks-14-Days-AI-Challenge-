# Databricks 14 Days AI Challenge ğŸš€

[![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](https://databricks.com)
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![PySpark](https://img.shields.io/badge/PySpark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)](https://spark.apache.org)

A comprehensive 14-day journey through Databricks, covering data engineering, machine learning, and AI capabilities on the Databricks Data Intelligence Platform.

## ğŸ“š Table of Contents

- [Overview](#overview)
- [Challenge Structure](#challenge-structure)
- [Prerequisites](#prerequisites)
- [Setup Instructions](#setup-instructions)
- [Daily Challenges](#daily-challenges)
- [Project Structure](#project-structure)
- [Resources](#resources)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Overview

This repository contains solutions and learning materials for the **Databricks 14 Days AI Challenge** organized by the Indian Data Club. The challenge is designed to help you master Databricks fundamentals, advanced data engineering concepts, machine learning workflows, and generative AI applications.

## ğŸ—“ï¸ Challenge Structure

### Week 1: Foundations & Data Engineering
- **Day 1-2**: Databricks Fundamentals & Workspace Navigation
- **Day 3-4**: Delta Lake & Data Ingestion
- **Day 5-7**: Advanced PySpark & ETL Pipelines

### Week 2: Machine Learning & AI
- **Day 8-9**: MLflow & Model Training
- **Day 10-11**: Feature Engineering & AutoML
- **Day 12-13**: Generative AI with Mosaic AI
- **Day 14**: Capstone Project & Deployment

## ğŸ”§ Prerequisites

- Databricks Community Edition account or Databricks Trial
- Basic Python programming knowledge
- Understanding of SQL fundamentals
- Familiarity with data concepts (optional but helpful)

## ğŸš€ Setup Instructions

### 1. Databricks Account Setup

```bash
# Sign up for Databricks Community Edition
# Visit: https://community.cloud.databricks.com/

# Or get a trial account
# Visit: https://databricks.com/try-databricks
```

### 2. Clone This Repository

```bash
git clone https://github.com/yourusername/databricks-14days-challenge.git
cd databricks-14days-challenge
```

### 3. Import Notebooks to Databricks

1. Navigate to your Databricks workspace
2. Click on **Workspace** â†’ **Users** â†’ **Your Username**
3. Click **Import** â†’ **URL** or **File**
4. Import the notebooks from the `notebooks/` directory

### 4. Install Required Libraries

Create a cluster and install the following libraries:
```python
# Requirements are specified in requirements.txt
pandas
numpy
matplotlib
seaborn
mlflow
scikit-learn
```

## ğŸ“– Daily Challenges

### Day 1: Introduction to Databricks
**Topics**: Workspace navigation, notebooks, clusters
- [ ] Create your first notebook
- [ ] Understand workspace structure
- [ ] Create and configure a cluster

ğŸ“ **Notebook**: `notebooks/day01_databricks_intro.ipynb`

### Day 2: Databricks File System (DBFS)
**Topics**: DBFS, data storage, file operations
- [ ] Explore DBFS structure
- [ ] Upload and manage datasets
- [ ] Practice file operations

ğŸ“ **Notebook**: `notebooks/day02_dbfs_operations.ipynb`

### Day 3: Introduction to Delta Lake
**Topics**: Delta tables, ACID transactions, time travel
- [ ] Create Delta tables
- [ ] Perform CRUD operations
- [ ] Use time travel features

ğŸ“ **Notebook**: `notebooks/day03_delta_lake_basics.ipynb`

### Day 4: Data Ingestion Patterns
**Topics**: Batch and streaming ingestion, Auto Loader
- [ ] Implement batch data loading
- [ ] Set up streaming pipelines
- [ ] Use Auto Loader for cloud storage

ğŸ“ **Notebook**: `notebooks/day04_data_ingestion.ipynb`

### Day 5: PySpark Fundamentals
**Topics**: DataFrames, transformations, actions
- [ ] Master DataFrame operations
- [ ] Apply transformations
- [ ] Understand lazy evaluation

ğŸ“ **Notebook**: `notebooks/day05_pyspark_basics.ipynb`

### Day 6: Advanced PySpark
**Topics**: Window functions, UDFs, optimization
- [ ] Use window functions
- [ ] Create custom UDFs
- [ ] Optimize Spark jobs

ğŸ“ **Notebook**: `notebooks/day06_advanced_pyspark.ipynb`

### Day 7: Building ETL Pipelines
**Topics**: Delta Live Tables, pipeline orchestration
- [ ] Design ETL workflows
- [ ] Implement Delta Live Tables
- [ ] Schedule and monitor pipelines

ğŸ“ **Notebook**: `notebooks/day07_etl_pipelines.ipynb`

### Day 8: Machine Learning Basics
**Topics**: ML workflows, feature engineering
- [ ] Prepare data for ML
- [ ] Split train/test datasets
- [ ] Create feature pipelines

ğŸ“ **Notebook**: `notebooks/day08_ml_basics.ipynb`

### Day 9: MLflow Integration
**Topics**: Experiment tracking, model registry
- [ ] Track experiments with MLflow
- [ ] Log parameters and metrics
- [ ] Register models

ğŸ“ **Notebook**: `notebooks/day09_mlflow_tracking.ipynb`

### Day 10: AutoML
**Topics**: Automated machine learning, model selection
- [ ] Use Databricks AutoML
- [ ] Compare model performance
- [ ] Deploy best models

ğŸ“ **Notebook**: `notebooks/day10_automl.ipynb`

### Day 11: Feature Store
**Topics**: Feature engineering, feature store, reusability
- [ ] Create feature tables
- [ ] Use Feature Store for training
- [ ] Ensure feature consistency

ğŸ“ **Notebook**: `notebooks/day11_feature_store.ipynb`

### Day 12: Generative AI Foundations
**Topics**: LLMs, embeddings, vector databases
- [ ] Understand LLM concepts
- [ ] Generate embeddings
- [ ] Work with Vector Search

ğŸ“ **Notebook**: `notebooks/day12_genai_foundations.ipynb`

### Day 13: RAG Applications
**Topics**: Retrieval-Augmented Generation, Mosaic AI
- [ ] Build RAG pipelines
- [ ] Use Mosaic AI Playground
- [ ] Create AI applications

ğŸ“ **Notebook**: `notebooks/day13_rag_applications.ipynb`

### Day 14: Capstone Project
**Topics**: End-to-end data & AI project
- [ ] Design complete solution
- [ ] Implement data pipeline
- [ ] Deploy ML/AI model
- [ ] Create dashboard/visualization

ğŸ“ **Notebook**: `notebooks/day14_capstone_project.ipynb`

## ğŸ“ Project Structure

```
databricks-14days-challenge/
â”‚
â”œâ”€â”€ notebooks/                      # Daily challenge notebooks
â”‚   â”œâ”€â”€ day01_databricks_intro.ipynb
â”‚   â”œâ”€â”€ day02_dbfs_operations.ipynb
â”‚   â”œâ”€â”€ day03_delta_lake_basics.ipynb
â”‚   â”œâ”€â”€ day04_data_ingestion.ipynb
â”‚   â”œâ”€â”€ day05_pyspark_basics.ipynb
â”‚   â”œâ”€â”€ day06_advanced_pyspark.ipynb
â”‚   â”œâ”€â”€ day07_etl_pipelines.ipynb
â”‚   â”œâ”€â”€ day08_ml_basics.ipynb
â”‚   â”œâ”€â”€ day09_mlflow_tracking.ipynb
â”‚   â”œâ”€â”€ day10_automl.ipynb
â”‚   â”œâ”€â”€ day11_feature_store.ipynb
â”‚   â”œâ”€â”€ day12_genai_foundations.ipynb
â”‚   â”œâ”€â”€ day13_rag_applications.ipynb
â”‚   â””â”€â”€ day14_capstone_project.ipynb
â”‚
â”œâ”€â”€ data/                           # Sample datasets
â”‚   â”œâ”€â”€ sample_data.csv
â”‚   â”œâ”€â”€ sales_data.json
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ src/                            # Python modules and utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ config/                         # Configuration files
â”‚   â”œâ”€â”€ cluster_config.json
â”‚   â””â”€â”€ pipeline_config.yaml
â”‚
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ setup_guide.md
â”‚   â”œâ”€â”€ best_practices.md
â”‚   â””â”€â”€ troubleshooting.md
â”‚
â”œâ”€â”€ tests/                          # Unit tests
â”‚   â”œâ”€â”€ test_data_processing.py
â”‚   â””â”€â”€ test_feature_engineering.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸ“š Resources

### Official Databricks Documentation
- [Databricks Documentation](https://docs.databricks.com/)
- [Delta Lake Guide](https://docs.delta.io/)
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)

### Learning Materials
- [Databricks Academy](https://www.databricks.com/learn/training)
- [Databricks Community Edition](https://community.cloud.databricks.com/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)

### Community
- [Indian Data Club](https://indiandataclub.com)
- [Databricks Community Forums](https://community.databricks.com/)
- [Stack Overflow - Databricks Tag](https://stackoverflow.com/questions/tagged/databricks)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes:

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Indian Data Club** for organizing this challenge
- **Databricks** for providing the platform and resources
- All contributors and participants in this learning journey

## ğŸ“§ Contact

For questions or feedback:
- Create an issue in this repository
- Join the Indian Data Club community
- Connect on LinkedIn

---

**Happy Learning! ğŸ‰**

*Start your Databricks journey today and become a Data & AI expert!*
